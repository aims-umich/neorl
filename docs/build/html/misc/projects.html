

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Projects &mdash; NEORL 1.1.3b documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme_overrides.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Contributors" href="contrib.html" />
    <link rel="prev" title="Changelog" href="changelog.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> NEORL
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                main (1.1.3b )
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/quickstart.html">Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/a2c.html">Advantage Actor Critic (A2C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/dqn.html">Deep Q Learning (DQN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/ppo2.html">Proximal Policy Optimisation (PPO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/ga.html">Genetic Algorithms (GA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/es.html">Evolution Strategies (ES)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/pso.html">Particle Swarm Optimisation (PSO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/de.html">Differential Evolution (DE)</a></li>
</ul>
<p class="caption"><span class="caption-text">Hyperparameter Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tune/grid.html">Grid Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/random.html">Random Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/evolu.html">Evolutionary Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/bayes.html">Bayesian Search</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex1.html">Example 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex2.html">Example 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex3.html">Example 3</a></li>
</ul>
<p class="caption"><span class="caption-text">Misc</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Projects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#physics-informed-reinforcement-learning-optimisation-with-neorl">Physics-informed Reinforcement Learning Optimisation with NEORL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reinforcement-learning-guiding-evolutionary-algorithms-in-constrained-optimization">Reinforcement Learning Guiding Evolutionary Algorithms in Constrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-of-ga-and-dqn-for-in-core-fuel-management">Application of GA and DQN for in-core fuel management</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contrib.html">Contributors</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NEORL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Projects</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/misc/projects.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="projects">
<span id="id1"></span><h1>Projects<a class="headerlink" href="#projects" title="Permalink to this headline">¶</a></h1>
<p>This is a list of projects using NEORL. Please contact us if you want your project to appear on this page.</p>
<div class="section" id="physics-informed-reinforcement-learning-optimisation-with-neorl">
<h2>Physics-informed Reinforcement Learning Optimisation with NEORL<a class="headerlink" href="#physics-informed-reinforcement-learning-optimisation-with-neorl" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/ge10x10.png"><img alt="alternate text" class="align-right" src="../_images/ge10x10.png" style="width: 245.5px; height: 280.5px;" /></a>
<p>Optimization of nuclear fuel assemblies if performed effectively, will lead to fuel efficiency improvement, cost reduction, and safety assurance. However, assembly optimization involves solving high-dimensional and computationally expensive combinatorial problems. As such, fuel designers’ expert judgement has commonly prevailed over the use of stochastic optimization (SO) algorithms such as genetic algorithms and simulated annealing. To improve the state-of-art, we explore a class of artificial intelligence (AI) algorithms, namely, reinforcement learning (RL) in this work. We propose a physics-informed AI optimization methodology by establishing a connection through reward shaping between RL and the tactics fuel designers follow in practice by moving fuel rods in the assembly to meet specific constraints and objectives. The methodology utilizes RL algorithms, deep Q learning and proximal policy optimization, and compares their performance to SO algorithms. The methodology is applied on two boiling water reactor assemblies of low-dimensional ( combinations) and high-dimensional ( combinations) natures. The results demonstrate that RL is more effective than SO in solving high dimensional problems, i.e., 10 × 10 assembly, through embedding expert knowledge in form of game rules and effectively exploring the search space. For a given computational resources and timeframe relevant to fuel designers, RL algorithms outperformed SO through finding more feasible patterns, 4–5 times more than SO, and through increasing search speed, as indicated by the RL outstanding computational efficiency. The results of this work clearly demonstrate RL effectiveness as another decision support tool for nuclear fuel assembly optimization.</p>
<div class="line-block">
<div class="line">Authors: Majdi I. Radaideh et al., 2021.</div>
<div class="line">Reference: <a class="reference external" href="https://doi.org/10.1016/j.nucengdes.2020.110966">https://doi.org/10.1016/j.nucengdes.2020.110966</a></div>
</div>
</div>
<div class="section" id="reinforcement-learning-guiding-evolutionary-algorithms-in-constrained-optimization">
<h2>Reinforcement Learning Guiding Evolutionary Algorithms in Constrained Optimization<a class="headerlink" href="#reinforcement-learning-guiding-evolutionary-algorithms-in-constrained-optimization" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/rlea.png"><img alt="alternate text" class="align-right" src="../_images/rlea.png" style="width: 288.5px; height: 401.5px;" /></a>
<p>For practical engineering optimization problems, the design space is typically narrow, given all the real-world constraints. Reinforcement Learning (RL) has commonly been guided by stochastic algorithms to tune hyperparameters and leverage exploration. Conversely in this work, we propose a rule-based RL methodology to guide evolutionary algorithms (EA) in constrained optimization. First, RL proximal policy optimization agents are trained to master matching some of the problem rules/constraints, then RL is used to inject experiences to guide various evolutionary/stochastic algorithms such as genetic algorithms, simulated annealing, particle swarm optimization, differential evolution, and natural evolution strategies. Accordingly, we develop RL-guided EAs, which are benchmarked against their standalone counterparts. RL-guided EA in continuous optimization demonstrates significant improvement over standalone EA for two engineering benchmarks. The main problem analyzed is nuclear fuel assembly combinatorial optimization with high-dimensional and computationally expensive physics. The results demonstrate the ability of RL to efficiently learn the rules that nuclear fuel engineers follow to realize candidate solutions. Without these rules, the design space is large for RL/EA to find many candidates. With imposing the rule-based RL methodology, we found that RL-guided EA outperforms standalone algorithms by a wide margin, with  times improvement in exploration capabilities and computational efficiency. These insights imply that when facing a constrained problem with numerous local optima, RL can be useful in focusing the search space in the areas where expert knowledge has demonstrated merit, while evolutionary/stochastic algorithms utilize their exploratory features to improve the number of feasible solutions.</p>
<div class="line-block">
<div class="line">Authors: Majdi I. Radaideh and Koroush Shirvan</div>
<div class="line">Reference: <a class="reference external" href="https://doi.org/10.1016/j.knosys.2021.106836">https://doi.org/10.1016/j.knosys.2021.106836</a></div>
</div>
</div>
<div class="section" id="application-of-ga-and-dqn-for-in-core-fuel-management">
<h2>Application of GA and DQN for in-core fuel management<a class="headerlink" href="#application-of-ga-and-dqn-for-in-core-fuel-management" title="Permalink to this headline">¶</a></h2>
<p>The nuclear reactor core is composed of few hundred assemblies. The loading of these assemblies is done with the goal of reducing its overall cost while maintaining safety limits. Typically, the core designers choose a unique position and fuel enrichment for each assembly through use of expert judgement. In this thesis, alternatives to the current core reload design process are explored. Genetic algorithm and deep Q-learning are applied in an attempt to reduce core design time and improve the final core layout. The reference core represents a 4-loop pressurized water reactor where fixed number of fuel enrichments and burnable poison distributions are assumed. The algorithms automatically shuffles the assembly positions to find the optimum loading pattern. It is determined that both algorithms are able to successfully start with a poorly performing core loading pattern and discover a well performing one, by the metrics of boron concentration, cycle exposure, enthalpy-rise factor, and pin power peaking. This shows potential for further applications of these algorithms for core design with a more expanded search space.</p>
<div class="line-block">
<div class="line">Author: Jane Reed</div>
<div class="line">Reference: <a class="reference external" href="http://34.201.211.163/handle/1721.1/127308">http://34.201.211.163/handle/1721.1/127308</a></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="contrib.html" class="btn btn-neutral float-right" title="Contributors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="changelog.html" class="btn btn-neutral float-left" title="Changelog" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Exelon Corp. &amp; MIT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>